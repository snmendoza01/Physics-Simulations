{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $x^4$ Approximation \n",
    "\n",
    "### __Theory:__\n",
    "The problem revolves around solving the SchrÃ¶dinger Equation for $x^4$ perturbation:\n",
    "\n",
    "$$ \\left(\\frac{1}{2m}\\hat{p}^2 + \\frac{1}{2}k\\hat{x}^2 + \\lambda x^4\\right)\\ket{\\psi} = E\\ket{\\psi} \\iff (\\hat{H}_\\text{osc} + \\lambda \\hat{x}^4)\\ket{\\psi} = E\\ket{\\psi}$$\n",
    "\n",
    "We will solve this equation for the ground state, which means finding the lowest eigenvalue $E$ and its corresponding eigenstate $\\ket{\\psi}$.\n",
    "\n",
    "Since the eigenstates of the QHO $\\,\\,\\{\\ket{n}\\}_{n=1}^\\infty\\,\\,$  form a basis for $L^2(\\mathbb{C})$, we can assume that the groundstate takes the form:\n",
    "\n",
    "$$\\ket{\\psi_0} = \\sum_{n = 0}^\\infty \\alpha _n \\ket{n} \\qquad \\alpha_i \\in \\mathbb{C}$$\n",
    "\n",
    "which reduces the problem to that of finding the coefficients $\\{\\alpha_i\\}_{i = 0}^\\infty \\,\\,$. This can be presented as an optimization problem: It can be shown that $$E_0 = \\bra{\\psi_0}\\hat{H}\\ket{\\psi_0} \\leq \\bra{\\psi_\\Theta}\\hat{H}\\ket{\\psi_\\Theta}$$ where $\\psi_\\Theta$ is a state perturbed away from $\\psi_0$. Thus, the loss functional $\\mathfrak{L}$ that we will try to minimize such that $\\ket{\\psi_\\Theta}$ approaches $\\ket{\\psi_0}$ is simply given by $$\\mathfrak{L}(\\ket{\\psi_\\Theta}) = \\bra{\\psi_\\Theta}\\hat{H}\\ket{\\psi_\\Theta}$$. \n",
    "\n",
    "### __Approach:__\n",
    "We will use pytorch to solve this problem. First, we will define a truncation parameter $N$ to turn the infinite representation of $\\ket{\\psi}$ into a finite approximate representation. The problem reduces to findining:\n",
    "$$\\boldsymbol{\\hat{\\alpha}} \\coloneqq \n",
    "\\underset{\\alpha }{\\arg\\min} |\\sum_{i,\\, j = 0}^N \\bar{\\alpha}_i \\alpha_j\\bra{i}\\hat{H}\\ket{j}|$$\n",
    "\n",
    "Given that this is now a finite dimensional problem, we can represent $\\hat{H}$ as a matrix $H \\eqqcolon [\\hat{H}]_\\mathfrak{B}$ (with respect to the basis $\\mathfrak{B}=\\{\\ket{0}, \\ket{1},...\\ket{N}\\}$ ), with its components given by $H_{nm} = \\bra{n}\\hat{H}\\ket{m}$. In this representation, \n",
    "$\\sum_{n=0}^N \\alpha_n \\ket{n} = \\boldsymbol{\\alpha}$ and thus \n",
    "$$\\boldsymbol{\\hat{\\alpha}} = \\underset{\\alpha }{\\arg\\min}|\\boldsymbol{\\alpha}^\\dagger H \\alpha|$$\n",
    "\n",
    "We can see that $[\\hat{H}_\\text{osc}]_\\mathfrak{B} = \\text{Diag}(e_0, e_1, ..., e_N)$ with $e_n = (\\frac{1}{2}+n)h\\omega$ as the basis elements are eigenstates of the operator. The representation of $\\hat{x}$ is not as obvious, but decomposing it into the ladder operators as $\\hat{x} = \\frac{1}{\\sqrt{2}}(\\hat{a}^\\dagger + \\hat{a})$, we have:\n",
    "$$[\\hat{x}]_\\mathfrak{B} = \\frac{1}{\\sqrt{2}}\n",
    "\\begin{pmatrix}\n",
    "    0 & \\sqrt{1} & 0 & 0 & \\dots\\\\\n",
    "    \\sqrt{1}& 0 & \\sqrt{2} & 0 &\\dots\\\\\n",
    "    0& \\sqrt{2}&0&\\sqrt{3}\\\\\n",
    "    0&0&\\sqrt{3}&0\\\\\n",
    "    \\dots&&&&{\\dots}\\\\\n",
    "    &&&&&\\sqrt{N}\\\\ \n",
    "    &&&&\\sqrt{N}&0\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Therefore, we are able to compute the coefficients by calculating the loss explicitely using the QHO basis representation for matrices and vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# hyperparameters\n",
    "w = 1\n",
    "h = 1\n",
    "lmbda = 0.25\n",
    "N = 50\n",
    "lr = 1e-3\n",
    "epochs = 40000\n",
    "\n",
    "hyperparams = {'lmbda': lmbda, 'lr': lr, 'epochs': epochs}\n",
    "\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to define the different tensors as outlined in the 'Theory' section, being careful to specify the vectors that are meant to have a gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = torch.rand(N+1, dtype=torch.cfloat, requires_grad=True)\n",
    "normed_coeffs = coeffs/torch.norm(coeffs)\n",
    "coeffs_conj = torch.conj(normed_coeffs)\n",
    "H_osc = torch.diag((torch.arange(N+1)+0.5)*h*w+0j)\n",
    "x  = torch.zeros(N+1, N+1, requires_grad=False, dtype=torch.cfloat)\n",
    "for i in range(N):\n",
    "    n = i+1\n",
    "    x[n][i] = math.sqrt(n/2)\n",
    "    x[i][n] = math.sqrt(n/2)\n",
    "\n",
    "# print(f'H matrix:\\n {H_osc}\\n')\n",
    "# print(f'x matrix:\\n {x}\\n')\n",
    "# print(f'alpha:\\n {normed_coeffs}\\n')\n",
    "# print(f'alpha conjugated:\\n {coeffs_conj}\\n')\n",
    "# print(f'Normalized? -> norm: {coeffs_conj@normed_coeffs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the loss function $\\mathfrak{L}$ as `x4loss`, along with an `optimize` function to optimize the coefficients. `optimize` will return data points for plotting the loss vs the number of iterations.\n",
    "\n",
    "_Note:_ the `x4loss` is a pure function, while optimize is not. This is made for simplicity in this scenario, as there is not much room for ambiguity with respect to the loss function. A more robust scenario would include the loss function as an argument, along with any arguments the loss function requires, but will be avoided in this situation for better readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 \n",
      "\n",
      "loss: 0.6941936612129211\n",
      "step: 50 \n",
      "\n",
      "loss: 0.6851170659065247\n",
      "step: 100 \n",
      "\n",
      "loss: 0.6776711940765381\n",
      "step: 150 \n",
      "\n",
      "loss: 0.6713687181472778\n",
      "step: 200 \n",
      "\n",
      "loss: 0.6659184694290161\n",
      "step: 250 \n",
      "\n",
      "loss: 0.6611543893814087\n",
      "step: 300 \n",
      "\n",
      "loss: 0.6569592952728271\n",
      "step: 350 \n",
      "\n",
      "loss: 0.6532407999038696\n",
      "step: 400 \n",
      "\n",
      "loss: 0.6499248147010803\n",
      "step: 450 \n",
      "\n",
      "loss: 0.6469520330429077\n",
      "step: 500 \n",
      "\n",
      "loss: 0.6442756652832031\n",
      "step: 550 \n",
      "\n",
      "loss: 0.6418589353561401\n",
      "step: 600 \n",
      "\n",
      "loss: 0.6396728157997131\n",
      "step: 650 \n",
      "\n",
      "loss: 0.637692928314209\n",
      "step: 700 \n",
      "\n",
      "loss: 0.6358990669250488\n",
      "step: 750 \n",
      "\n",
      "loss: 0.6342742443084717\n",
      "step: 800 \n",
      "\n",
      "loss: 0.6328034400939941\n",
      "step: 850 \n",
      "\n",
      "loss: 0.631473183631897\n",
      "step: 900 \n",
      "\n",
      "loss: 0.630271315574646\n",
      "step: 950 \n",
      "\n",
      "loss: 0.6291874647140503\n",
      "step: 1000 \n",
      "\n",
      "loss: 0.628211259841919\n",
      "step: 1050 \n",
      "\n",
      "loss: 0.627332866191864\n",
      "step: 1100 \n",
      "\n",
      "loss: 0.6265453100204468\n",
      "step: 1150 \n",
      "\n",
      "loss: 0.6258437633514404\n",
      "step: 1200 \n",
      "\n",
      "loss: 0.6252105236053467\n",
      "step: 1250 \n",
      "\n",
      "loss: 0.6246490478515625\n",
      "step: 1300 \n",
      "\n",
      "loss: 0.6241504549980164\n",
      "step: 1350 \n",
      "\n",
      "loss: 0.6237100958824158\n",
      "step: 1400 \n",
      "\n",
      "loss: 0.6233189105987549\n",
      "step: 1450 \n",
      "\n",
      "loss: 0.6229764819145203\n",
      "step: 1500 \n",
      "\n",
      "loss: 0.622674286365509\n",
      "step: 1550 \n",
      "\n",
      "loss: 0.6224157214164734\n",
      "step: 1600 \n",
      "\n",
      "loss: 0.6221826672554016\n",
      "step: 1650 \n",
      "\n",
      "loss: 0.6219838857650757\n",
      "step: 1700 \n",
      "\n",
      "loss: 0.621812641620636\n",
      "step: 1750 \n",
      "\n",
      "loss: 0.6216689348220825\n",
      "step: 1800 \n",
      "\n",
      "loss: 0.6215397119522095\n",
      "step: 1850 \n",
      "\n",
      "loss: 0.6214332580566406\n",
      "step: 1900 \n",
      "\n",
      "loss: 0.6213423609733582\n",
      "step: 1950 \n",
      "\n",
      "loss: 0.6212682723999023\n",
      "step: 2000 \n",
      "\n",
      "loss: 0.6212021112442017\n",
      "step: 2050 \n",
      "\n",
      "loss: 0.621148943901062\n",
      "step: 2100 \n",
      "\n",
      "loss: 0.6211051344871521\n",
      "step: 2150 \n",
      "\n",
      "loss: 0.6210708618164062\n",
      "step: 2200 \n",
      "\n",
      "loss: 0.621039628982544\n",
      "step: 2250 \n",
      "\n",
      "loss: 0.6210154294967651\n",
      "step: 2300 \n",
      "\n",
      "loss: 0.6209961771965027\n",
      "step: 2350 \n",
      "\n",
      "loss: 0.6209806799888611\n",
      "step: 2400 \n",
      "\n",
      "loss: 0.6209689974784851\n",
      "step: 2450 \n",
      "\n",
      "loss: 0.620958685874939\n",
      "step: 2500 \n",
      "\n",
      "loss: 0.6209514141082764\n",
      "step: 2550 \n",
      "\n",
      "loss: 0.6209450960159302\n",
      "step: 2600 \n",
      "\n",
      "loss: 0.6209405660629272\n",
      "step: 2650 \n",
      "\n",
      "loss: 0.6209371089935303\n",
      "step: 2700 \n",
      "\n",
      "loss: 0.6209344267845154\n",
      "step: 2750 \n",
      "\n",
      "loss: 0.6209324598312378\n",
      "step: 2800 \n",
      "\n",
      "loss: 0.6209312677383423\n",
      "step: 2850 \n",
      "\n",
      "loss: 0.6209297776222229\n",
      "step: 2900 \n",
      "\n",
      "loss: 0.6209290027618408\n",
      "step: 2950 \n",
      "\n",
      "loss: 0.6209284663200378\n",
      "step: 3000 \n",
      "\n",
      "loss: 0.6209360361099243\n",
      "step: 3050 \n",
      "\n",
      "loss: 0.620927631855011\n",
      "step: 3100 \n",
      "\n",
      "loss: 0.6209275722503662\n",
      "step: 3150 \n",
      "\n",
      "loss: 0.6209273934364319\n",
      "step: 3200 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 3250 \n",
      "\n",
      "loss: 0.6209327578544617\n",
      "step: 3300 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 3350 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 3400 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 3450 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 3500 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 3550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 3600 \n",
      "\n",
      "loss: 0.6209276914596558\n",
      "step: 3650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 3700 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 3750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 3800 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 3850 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 3900 \n",
      "\n",
      "loss: 0.6209279298782349\n",
      "step: 3950 \n",
      "\n",
      "loss: 0.6209362745285034\n",
      "step: 4000 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 4050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 4100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 4150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 4200 \n",
      "\n",
      "loss: 0.6209279298782349\n",
      "step: 4250 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 4300 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 4350 \n",
      "\n",
      "loss: 0.6209301948547363\n",
      "step: 4400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 4450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 4500 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 4550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 4600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 4650 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 4700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 4750 \n",
      "\n",
      "loss: 0.6209350824356079\n",
      "step: 4800 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 4850 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 4900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 4950 \n",
      "\n",
      "loss: 0.6209281086921692\n",
      "step: 5000 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 5050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 5100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 5150 \n",
      "\n",
      "loss: 0.6209272742271423\n",
      "step: 5200 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 5250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 5300 \n",
      "\n",
      "loss: 0.6209287643432617\n",
      "step: 5350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 5400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 5450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 5500 \n",
      "\n",
      "loss: 0.6209374666213989\n",
      "step: 5550 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 5600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 5650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 5700 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 5750 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 5800 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 5850 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 5900 \n",
      "\n",
      "loss: 0.6209274530410767\n",
      "step: 5950 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 6000 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 6050 \n",
      "\n",
      "loss: 0.6209273934364319\n",
      "step: 6100 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 6150 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6200 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6250 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6300 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 6350 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6400 \n",
      "\n",
      "loss: 0.6209297180175781\n",
      "step: 6450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6550 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6600 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6650 \n",
      "\n",
      "loss: 0.6209303140640259\n",
      "step: 6700 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 6750 \n",
      "\n",
      "loss: 0.6209283471107483\n",
      "step: 6800 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6850 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 6950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 7000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 7050 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 7100 \n",
      "\n",
      "loss: 0.620928168296814\n",
      "step: 7150 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 7200 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 7250 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 7300 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 7350 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 7400 \n",
      "\n",
      "loss: 0.6209288835525513\n",
      "step: 7450 \n",
      "\n",
      "loss: 0.6209285259246826\n",
      "step: 7500 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 7550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 7600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 7650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 7700 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 7750 \n",
      "\n",
      "loss: 0.6209306716918945\n",
      "step: 7800 \n",
      "\n",
      "loss: 0.6209440231323242\n",
      "step: 7850 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 7900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 7950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 8000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 8050 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 8100 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 8150 \n",
      "\n",
      "loss: 0.6209274530410767\n",
      "step: 8200 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 8250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 8300 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 8350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 8400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 8450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 8500 \n",
      "\n",
      "loss: 0.620928168296814\n",
      "step: 8550 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 8600 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 8650 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 8700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 8750 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 8800 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 8850 \n",
      "\n",
      "loss: 0.6209337711334229\n",
      "step: 8900 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 8950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 9000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 9050 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 9100 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 9150 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 9200 \n",
      "\n",
      "loss: 0.6209280490875244\n",
      "step: 9250 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 9300 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 9350 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 9400 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 9450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 9500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 9550 \n",
      "\n",
      "loss: 0.6209326982498169\n",
      "step: 9600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 9650 \n",
      "\n",
      "loss: 0.6209288239479065\n",
      "step: 9700 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 9750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 9800 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 9850 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 9900 \n",
      "\n",
      "loss: 0.6209282875061035\n",
      "step: 9950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 10000 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 10050 \n",
      "\n",
      "loss: 0.6209286451339722\n",
      "step: 10100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 10150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 10200 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 10250 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 10300 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 10350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 10400 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 10450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 10500 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 10550 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 10600 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 10650 \n",
      "\n",
      "loss: 0.6209276914596558\n",
      "step: 10700 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 10750 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 10800 \n",
      "\n",
      "loss: 0.6209273934364319\n",
      "step: 10850 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 10900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 10950 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 11000 \n",
      "\n",
      "loss: 0.6209278106689453\n",
      "step: 11050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 11100 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 11150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 11200 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 11250 \n",
      "\n",
      "loss: 0.6209279298782349\n",
      "step: 11300 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 11350 \n",
      "\n",
      "loss: 0.6209276914596558\n",
      "step: 11400 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 11450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 11500 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 11550 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 11600 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 11650 \n",
      "\n",
      "loss: 0.6209286451339722\n",
      "step: 11700 \n",
      "\n",
      "loss: 0.6209274530410767\n",
      "step: 11750 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 11800 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 11850 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 11900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 11950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 12000 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 12050 \n",
      "\n",
      "loss: 0.62093186378479\n",
      "step: 12100 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 12150 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 12200 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 12250 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 12300 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 12350 \n",
      "\n",
      "loss: 0.6209290027618408\n",
      "step: 12400 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 12450 \n",
      "\n",
      "loss: 0.6209291219711304\n",
      "step: 12500 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 12550 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 12600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 12650 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 12700 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 12750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 12800 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 12850 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 12900 \n",
      "\n",
      "loss: 0.620927631855011\n",
      "step: 12950 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 13000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 13050 \n",
      "\n",
      "loss: 0.6209278106689453\n",
      "step: 13100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 13150 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 13200 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 13250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 13300 \n",
      "\n",
      "loss: 0.6209293603897095\n",
      "step: 13350 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 13400 \n",
      "\n",
      "loss: 0.6209293603897095\n",
      "step: 13450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 13500 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 13550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 13600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 13650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 13700 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 13750 \n",
      "\n",
      "loss: 0.6209291219711304\n",
      "step: 13800 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 13850 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 13900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 13950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 14000 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 14050 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 14100 \n",
      "\n",
      "loss: 0.6209279894828796\n",
      "step: 14150 \n",
      "\n",
      "loss: 0.6209278702735901\n",
      "step: 14200 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 14250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 14300 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 14350 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 14400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 14450 \n",
      "\n",
      "loss: 0.6209279298782349\n",
      "step: 14500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 14550 \n",
      "\n",
      "loss: 0.6209285259246826\n",
      "step: 14600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 14650 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 14700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 14750 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 14800 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 14850 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 14900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 14950 \n",
      "\n",
      "loss: 0.6209279298782349\n",
      "step: 15000 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 15050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 15100 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 15150 \n",
      "\n",
      "loss: 0.6209275722503662\n",
      "step: 15200 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 15250 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 15300 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 15350 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 15400 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 15450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 15500 \n",
      "\n",
      "loss: 0.6209272742271423\n",
      "step: 15550 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 15600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 15650 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 15700 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 15750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 15800 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 15850 \n",
      "\n",
      "loss: 0.6209288835525513\n",
      "step: 15900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 15950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 16000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 16050 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 16100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 16150 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 16200 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 16250 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 16300 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 16350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 16400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 16450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 16500 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 16550 \n",
      "\n",
      "loss: 0.6209410429000854\n",
      "step: 16600 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 16650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 16700 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 16750 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 16800 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 16850 \n",
      "\n",
      "loss: 0.6209272742271423\n",
      "step: 16900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 16950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 17000 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 17050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 17100 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 17150 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 17200 \n",
      "\n",
      "loss: 0.6209285855293274\n",
      "step: 17250 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 17300 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 17350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 17400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 17450 \n",
      "\n",
      "loss: 0.6209285259246826\n",
      "step: 17500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 17550 \n",
      "\n",
      "loss: 0.6209286451339722\n",
      "step: 17600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 17650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 17700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 17750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 17800 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 17850 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 17900 \n",
      "\n",
      "loss: 0.6209290027618408\n",
      "step: 17950 \n",
      "\n",
      "loss: 0.6209273934364319\n",
      "step: 18000 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 18050 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 18100 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 18150 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 18200 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 18250 \n",
      "\n",
      "loss: 0.6209285259246826\n",
      "step: 18300 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 18350 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 18400 \n",
      "\n",
      "loss: 0.6209276914596558\n",
      "step: 18450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 18500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 18550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 18600 \n",
      "\n",
      "loss: 0.620933473110199\n",
      "step: 18650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 18700 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 18750 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 18800 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 18850 \n",
      "\n",
      "loss: 0.6209275722503662\n",
      "step: 18900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 18950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 19000 \n",
      "\n",
      "loss: 0.6209290027618408\n",
      "step: 19050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 19100 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 19150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 19200 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 19250 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 19300 \n",
      "\n",
      "loss: 0.6209309101104736\n",
      "step: 19350 \n",
      "\n",
      "loss: 0.6209276914596558\n",
      "step: 19400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 19450 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 19500 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 19550 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 19600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 19650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 19700 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 19750 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 19800 \n",
      "\n",
      "loss: 0.6209352016448975\n",
      "step: 19850 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 19900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 19950 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 20000 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 20050 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 20100 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 20150 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 20200 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 20250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 20300 \n",
      "\n",
      "loss: 0.6209279298782349\n",
      "step: 20350 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 20400 \n",
      "\n",
      "loss: 0.620927631855011\n",
      "step: 20450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 20500 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 20550 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 20600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 20650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 20700 \n",
      "\n",
      "loss: 0.6209313869476318\n",
      "step: 20750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 20800 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 20850 \n",
      "\n",
      "loss: 0.620928168296814\n",
      "step: 20900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 20950 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 21000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 21050 \n",
      "\n",
      "loss: 0.6209399700164795\n",
      "step: 21100 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 21150 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 21200 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 21250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 21300 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 21350 \n",
      "\n",
      "loss: 0.6209454536437988\n",
      "step: 21400 \n",
      "\n",
      "loss: 0.6209280490875244\n",
      "step: 21450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 21500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 21550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 21600 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 21650 \n",
      "\n",
      "loss: 0.62093186378479\n",
      "step: 21700 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 21750 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 21800 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 21850 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 21900 \n",
      "\n",
      "loss: 0.6209309697151184\n",
      "step: 21950 \n",
      "\n",
      "loss: 0.620928168296814\n",
      "step: 22000 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 22050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 22100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 22150 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 22200 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 22250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 22300 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 22350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 22400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 22450 \n",
      "\n",
      "loss: 0.6209288835525513\n",
      "step: 22500 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 22550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 22600 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 22650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 22700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 22750 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 22800 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 22850 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 22900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 22950 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 23000 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 23050 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 23100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 23150 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 23200 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 23250 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 23300 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 23350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 23400 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 23450 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 23500 \n",
      "\n",
      "loss: 0.6209280490875244\n",
      "step: 23550 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 23600 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 23650 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 23700 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 23750 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 23800 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 23850 \n",
      "\n",
      "loss: 0.6209276914596558\n",
      "step: 23900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 23950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 24000 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 24050 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 24100 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 24150 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 24200 \n",
      "\n",
      "loss: 0.6209392547607422\n",
      "step: 24250 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 24300 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 24350 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 24400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 24450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 24500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 24550 \n",
      "\n",
      "loss: 0.6209330558776855\n",
      "step: 24600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 24650 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 24700 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 24750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 24800 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 24850 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 24900 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 24950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 25000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 25050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 25100 \n",
      "\n",
      "loss: 0.6209284067153931\n",
      "step: 25150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 25200 \n",
      "\n",
      "loss: 0.6209272742271423\n",
      "step: 25250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 25300 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 25350 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 25400 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 25450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 25500 \n",
      "\n",
      "loss: 0.6209279894828796\n",
      "step: 25550 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 25600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 25650 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 25700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 25750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 25800 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 25850 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 25900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 25950 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 26000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 26050 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 26100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 26150 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 26200 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 26250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 26300 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 26350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 26400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 26450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 26500 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 26550 \n",
      "\n",
      "loss: 0.6209288835525513\n",
      "step: 26600 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 26650 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 26700 \n",
      "\n",
      "loss: 0.6209274530410767\n",
      "step: 26750 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 26800 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 26850 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 26900 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 26950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 27000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 27050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 27100 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 27150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 27200 \n",
      "\n",
      "loss: 0.6209290027618408\n",
      "step: 27250 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 27300 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 27350 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 27400 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 27450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 27500 \n",
      "\n",
      "loss: 0.6209287643432617\n",
      "step: 27550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 27600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 27650 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 27700 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 27750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 27800 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 27850 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 27900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 27950 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 28000 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 28050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 28100 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 28150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 28200 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 28250 \n",
      "\n",
      "loss: 0.6209310293197632\n",
      "step: 28300 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 28350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 28400 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 28450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 28500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 28550 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 28600 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 28650 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 28700 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 28750 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 28800 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 28850 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 28900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 28950 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 29000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 29050 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 29100 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 29150 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 29200 \n",
      "\n",
      "loss: 0.6209290027618408\n",
      "step: 29250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 29300 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 29350 \n",
      "\n",
      "loss: 0.6209272742271423\n",
      "step: 29400 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 29450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 29500 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 29550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 29600 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 29650 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 29700 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 29750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 29800 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 29850 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 29900 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 29950 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 30000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 30050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 30100 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 30150 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 30200 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 30250 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 30300 \n",
      "\n",
      "loss: 0.6209287643432617\n",
      "step: 30350 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 30400 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 30450 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 30500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 30550 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 30600 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 30650 \n",
      "\n",
      "loss: 0.6209272742271423\n",
      "step: 30700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 30750 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 30800 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 30850 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 30900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 30950 \n",
      "\n",
      "loss: 0.6209279298782349\n",
      "step: 31000 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 31050 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 31100 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 31150 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 31200 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 31250 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 31300 \n",
      "\n",
      "loss: 0.6209284067153931\n",
      "step: 31350 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 31400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 31450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 31500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 31550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 31600 \n",
      "\n",
      "loss: 0.6209280490875244\n",
      "step: 31650 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 31700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 31750 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 31800 \n",
      "\n",
      "loss: 0.6209292411804199\n",
      "step: 31850 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 31900 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 31950 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 32000 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 32050 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 32100 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 32150 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 32200 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 32250 \n",
      "\n",
      "loss: 0.6209350228309631\n",
      "step: 32300 \n",
      "\n",
      "loss: 0.6209334135055542\n",
      "step: 32350 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 32400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 32450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 32500 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 32550 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 32600 \n",
      "\n",
      "loss: 0.6209306120872498\n",
      "step: 32650 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 32700 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 32750 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 32800 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 32850 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 32900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 32950 \n",
      "\n",
      "loss: 0.6209285259246826\n",
      "step: 33000 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 33050 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 33100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 33150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 33200 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 33250 \n",
      "\n",
      "loss: 0.620938777923584\n",
      "step: 33300 \n",
      "\n",
      "loss: 0.6209274530410767\n",
      "step: 33350 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 33400 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 33450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 33500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 33550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 33600 \n",
      "\n",
      "loss: 0.6209280490875244\n",
      "step: 33650 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 33700 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 33750 \n",
      "\n",
      "loss: 0.6209280490875244\n",
      "step: 33800 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 33850 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 33900 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 33950 \n",
      "\n",
      "loss: 0.6209278106689453\n",
      "step: 34000 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 34050 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 34100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 34150 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 34200 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 34250 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 34300 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 34350 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 34400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 34450 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 34500 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 34550 \n",
      "\n",
      "loss: 0.6209299564361572\n",
      "step: 34600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 34650 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 34700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 34750 \n",
      "\n",
      "loss: 0.6209273934364319\n",
      "step: 34800 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 34850 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 34900 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 34950 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 35000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 35050 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 35100 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 35150 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 35200 \n",
      "\n",
      "loss: 0.6209309101104736\n",
      "step: 35250 \n",
      "\n",
      "loss: 0.6209298372268677\n",
      "step: 35300 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 35350 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 35400 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 35450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 35500 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 35550 \n",
      "\n",
      "loss: 0.6209359169006348\n",
      "step: 35600 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 35650 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 35700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 35750 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 35800 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 35850 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 35900 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 35950 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 36000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 36050 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 36100 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 36150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 36200 \n",
      "\n",
      "loss: 0.6209336519241333\n",
      "step: 36250 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 36300 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 36350 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 36400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 36450 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 36500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 36550 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 36600 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 36650 \n",
      "\n",
      "loss: 0.6209271550178528\n",
      "step: 36700 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 36750 \n",
      "\n",
      "loss: 0.6209276914596558\n",
      "step: 36800 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 36850 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 36900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 36950 \n",
      "\n",
      "loss: 0.6209331750869751\n",
      "step: 37000 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 37050 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 37100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 37150 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 37200 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 37250 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 37300 \n",
      "\n",
      "loss: 0.6209274530410767\n",
      "step: 37350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 37400 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 37450 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 37500 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 37550 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 37600 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 37650 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 37700 \n",
      "\n",
      "loss: 0.6209303140640259\n",
      "step: 37750 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 37800 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 37850 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 37900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 37950 \n",
      "\n",
      "loss: 0.6209282279014587\n",
      "step: 38000 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 38050 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 38100 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 38150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 38200 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 38250 \n",
      "\n",
      "loss: 0.6209268569946289\n",
      "step: 38300 \n",
      "\n",
      "loss: 0.6209287643432617\n",
      "step: 38350 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 38400 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 38450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 38500 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 38550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 38600 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 38650 \n",
      "\n",
      "loss: 0.6209275126457214\n",
      "step: 38700 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 38750 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 38800 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 38850 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 38900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 38950 \n",
      "\n",
      "loss: 0.6209282875061035\n",
      "step: 39000 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 39050 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 39100 \n",
      "\n",
      "loss: 0.6209341883659363\n",
      "step: 39150 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 39200 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 39250 \n",
      "\n",
      "loss: 0.6209272146224976\n",
      "step: 39300 \n",
      "\n",
      "loss: 0.6209277510643005\n",
      "step: 39350 \n",
      "\n",
      "loss: 0.6209269165992737\n",
      "step: 39400 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 39450 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 39500 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 39550 \n",
      "\n",
      "loss: 0.620927095413208\n",
      "step: 39600 \n",
      "\n",
      "loss: 0.6209272742271423\n",
      "step: 39650 \n",
      "\n",
      "loss: 0.6209273338317871\n",
      "step: 39700 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 39750 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 39800 \n",
      "\n",
      "loss: 0.6209270358085632\n",
      "step: 39850 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 39900 \n",
      "\n",
      "loss: 0.6209269762039185\n",
      "step: 39950 \n",
      "\n",
      "loss: 0.620927095413208\n"
     ]
    }
   ],
   "source": [
    "def x4loss(alpha, Hosc, x, lmbda):\n",
    "    normed_alpha = alpha/torch.norm(alpha)\n",
    "    alpha_conj = torch.conj(normed_alpha)\n",
    "    return torch.abs(alpha_conj@(Hosc + lmbda*x@x@x@x)@normed_alpha)\n",
    "\n",
    "def optimize(alpha, x, H_osc, **hyperparams):\n",
    "    lr = hyperparams['lr']\n",
    "    epochs = hyperparams['epochs']\n",
    "    lmbda = hyperparams['lmbda']\n",
    "    interv = 50\n",
    "    optimizing_info = torch.zeros(epochs//interv, 2)\n",
    "    optimizer = torch.optim.Adam([alpha], lr=lr)\n",
    "    for step in range(epochs):\n",
    "        loss = x4loss(coeffs, H_osc, x, lmbda)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not step%interv:\n",
    "            print(f'step: {step} \\n\\nloss: {loss}')\n",
    "            optimizing_info[step//interv][0] = step\n",
    "            optimizing_info[step//interv][1] = loss\n",
    "    return optimizing_info\n",
    "plot_data = optimize(coeffs, x, H_osc, **hyperparams).detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after roughly $10 000$ iterations, the loss stabilizes around a value $y\\approx0.62092$ for $\\lambda = 0.25$, which would imply that this is the ground state energy of the system. \n",
    "\n",
    "We will proceed to plot the values for the Loss along with this inferred lower bound for the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(plot_data[:,0], plot_data[:, 1], label='Loss', alpha=0.7)\n",
    "plt.plot(torch.arange(0, epochs), [0.62092]*epochs, ls='--', alpha=0.7, \n",
    "         label=r'$y=0.62092$')\n",
    "plt.title('Loss vs iterations')\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Loss function stabilized around step $10000$ indeed, suggesting the ground energy has been reached (or a local minimum for the Loss function), and it is equal to the aforementioned $y$. \n",
    "\n",
    "To analyze the eigenstate, we just need to analyze the coefficients that were optimized. They are shown below, both printed out and in a diagram. \n",
    "\n",
    "_Remark:_ The even terms of the basis appear to have a significant distribution, while the odd terms are orders of magnitude smaller than the even terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.3236e-01+6.6915e-01j,  8.7866e-12-4.3475e-11j,\n",
      "        -9.2765e-02-8.4758e-02j,  1.0216e-11-5.0547e-11j,\n",
      "         1.4402e-03+1.3163e-03j,  1.2537e-11-6.2033e-11j,\n",
      "         6.4689e-03+5.9110e-03j,  1.5896e-11-7.8648e-11j,\n",
      "        -3.2928e-03-3.0082e-03j,  2.0368e-11-1.0077e-10j,\n",
      "         9.5349e-04+8.7153e-04j,  2.5985e-11-1.2857e-10j,\n",
      "        -6.9716e-05-6.3428e-05j,  3.2700e-11-1.6179e-10j,\n",
      "        -1.1793e-04-1.0756e-04j,  4.0371e-11-1.9975e-10j,\n",
      "         9.3521e-05+8.5554e-05j,  4.8774e-11-2.4132e-10j,\n",
      "        -4.3397e-05-3.9628e-05j,  5.7619e-11-2.8509e-10j,\n",
      "         1.2493e-05+1.1371e-05j,  6.6589e-11-3.2947e-10j,\n",
      "         6.7719e-08-2.7840e-08j,  7.5361e-11-3.7287e-10j,\n",
      "        -2.9497e-06-2.8052e-06j,  8.3648e-11-4.1387e-10j,\n",
      "         2.4077e-06+2.0904e-06j,  9.1216e-11-4.5132e-10j,\n",
      "        -1.2480e-06-1.2349e-06j,  9.7903e-11-4.8440e-10j,\n",
      "         4.6109e-07+3.5307e-07j,  1.0362e-10-5.1271e-10j,\n",
      "        -5.7190e-08-8.6242e-08j,  1.0837e-10-5.3621e-10j,\n",
      "        -7.3045e-08-6.5454e-08j,  1.1220e-10-5.5516e-10j,\n",
      "         7.7368e-08+1.0446e-07j,  1.1520e-10-5.7000e-10j,\n",
      "        -5.8696e-08+9.9535e-09j,  1.1747e-10-5.8123e-10j,\n",
      "         1.7275e-08+1.0646e-07j,  1.1910e-10-5.8929e-10j,\n",
      "        -1.9336e-08+9.6566e-08j,  1.2017e-10-5.9460e-10j,\n",
      "        -1.1720e-08+1.2315e-07j,  1.2079e-10-5.9766e-10j,\n",
      "        -1.1248e-08+1.3829e-07j,  1.2108e-10-5.9908e-10j,\n",
      "        -1.8157e-08+1.3993e-07j,  1.2118e-10-5.9958e-10j,\n",
      "        -1.3841e-08+1.4670e-07j], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "normed_coeffs = coeffs/torch.norm(coeffs)\n",
    "print(normed_coeffs)\n",
    "epsi = 1e-13  # to avoid evaluating log(0)\n",
    "plt.scatter(torch.arange(0, N+1), torch.log10(torch.abs(normed_coeffs).detach()+epsi), s=10)\n",
    "plt.title(r'$x^4$ coefficients after stabilization')\n",
    "plt.xlabel(r'$n$')\n",
    "plt.ylabel(r'$\\log_{10}|\\alpha_n|$')\n",
    "plt.grid(ls='--')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c64bf8550111f692d1dbcc1d75814978e4e1f37e5d274065f0dbf4ce2794aed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
